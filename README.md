# GenAi_Assignment
<hr>
1. Introduces the Transformer model based on attention mechanisms for sequence transduction tasks.                <br>
2. Aims to replace complex recurrent or convolutional neural networks with a more parallelizable architecture.    <br>
3. Shows superior quality in machine translation tasks with faster training and more parallelization.             <br>
4. Achieves state-of-the-art results on translation tasks with lower training costs.                              <br>
5. Transformer relies on self-attention mechanisms to handle long-range dependencies effectively.                 <br>
6. Utilizes byte-pair or word-piece encoding for training data from standard datasets.                            <br>
7. Training conducted on hardware with 8 NVIDIA P100 GPUs with specific step times.                               <br>
8. Adam optimizer used with customized hyperparameters and learning rate schedule.                                <br>
9. Demonstrates improved BLEU scores on English-to-German and English-to-French translation tasks.                <br>
10. Provides an efficient and effective alternative model architecture for sequence transduction tasks.           <br>
<hr>
